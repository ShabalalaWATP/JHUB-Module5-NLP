{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not chance these dependencies\n",
    "import pytest\n",
    "# Import your dependencies here\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import spacy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbc_scraper(url):\n",
    "    \"\"\"\n",
    "    This function should take a url, which will relate to a bbc news article \n",
    "    and return a json object containing the following fields:\n",
    "    1) URL (provided.  For example https://www.bbc.co.uk/news/uk-51004218)\n",
    "    2) Title\n",
    "    3) Date_published\n",
    "    4) Content --(the main body of article, this must be one continuous string without linebreaks)\n",
    "    The function must be iterable (If placed in a for loop and provided with several URLs in \n",
    "    turn return the correct json object for each time it is invoked without any manual intervention)\n",
    "    \"\"\"\n",
    "    # Need this header or BBC blocks the request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get the webpage content\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the title in h1 tag\n",
    "        title_element = soup.find('h1')\n",
    "        title = title_element.get_text().strip() if title_element else \"Title not found\"\n",
    "        \n",
    "        # Sort out the date format\n",
    "        time_element = soup.find('time')\n",
    "        if time_element:\n",
    "            date = time_element.get('datetime', '')\n",
    "            try:\n",
    "                # Convert to readable format\n",
    "                date_obj = datetime.datetime.fromisoformat(date.replace('Z', '+00:00'))\n",
    "                date = date_obj.strftime('%-d %B %Y')\n",
    "            except:\n",
    "                date = time_element.get_text().strip()\n",
    "        else:\n",
    "            date = \"Date not found\"\n",
    "        \n",
    "        # Try different methods to get the content\n",
    "        content = []\n",
    "        \n",
    "        # Check these containers in order of likelihood\n",
    "        content_containers = [\n",
    "            soup.find('article'),  # Most likely place\n",
    "            soup.find('div', {'id': 'main-content'}),  # Second choice\n",
    "            soup.find('div', class_=['story-body', 'article-body']),  # Older article format\n",
    "            soup  # Last resort\n",
    "        ]\n",
    "        \n",
    "        # Use the first container that works\n",
    "        for container in content_containers:\n",
    "            if container:\n",
    "                paragraphs = container.find_all('p')\n",
    "                if paragraphs:\n",
    "                    # Clean up the paragraphs\n",
    "                    for p in paragraphs:\n",
    "                        # Things we want to skip\n",
    "                        skip_classes = ['tag', 'share', 'media', 'navigation']\n",
    "                        skip_phrases = ['Related Topics', 'More on this story', 'SIGN UP', 'Follow us']\n",
    "                        \n",
    "                        if not any(class_ in str(p.get('class', [])) for class_ in skip_classes):\n",
    "                            text = p.get_text().strip()\n",
    "                            if text and not any(phrase in text for phrase in skip_phrases):\n",
    "                                content.append(text)\n",
    "                    break  # Found what we need\n",
    "        \n",
    "        # Join everything into one continuous string\n",
    "        content_text = ' '.join(content)\n",
    "        content_text = ' '.join(content_text.split())\n",
    "        \n",
    "        # Create the final JSON\n",
    "        results_json = json.dumps({\n",
    "            'URL': url,\n",
    "            'Title': title,\n",
    "            'Date_published': date,\n",
    "            'Content': content_text\n",
    "        })\n",
    "        \n",
    "        return results_json\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return error if something goes wrong\n",
    "        return json.dumps({\n",
    "            'error': f\"Failed to scrape article: {str(e)}\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(string):\n",
    "    \"\"\"\n",
    "    This function should return a json containing the:\n",
    "    1) People\n",
    "    2) Places\n",
    "    3) Organisations \n",
    "    in the text string provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to load spacy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        # If model isn't downloaded, get it first\n",
    "        import subprocess\n",
    "        subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Process the text through spacy\n",
    "    doc = nlp(string)\n",
    "    \n",
    "    # Set up our collections\n",
    "    entities = {\n",
    "        'people': [],\n",
    "        'places': [],\n",
    "        'organisations': []\n",
    "    }\n",
    "    \n",
    "    # Sort through what we've found\n",
    "    for ent in doc.ents:\n",
    "        # Look for names of people\n",
    "        if ent.label_ in ['PERSON']:\n",
    "            if ent.text not in entities['people']:\n",
    "                entities['people'].append(ent.text)\n",
    "        # Look for places and locations\n",
    "        elif ent.label_ in ['GPE', 'LOC']:\n",
    "            if ent.text not in entities['places']:\n",
    "                entities['places'].append(ent.text)\n",
    "        # Look for organisations\n",
    "        elif ent.label_ in ['ORG']:\n",
    "            if ent.text not in entities['organisations']:\n",
    "                entities['organisations'].append(ent.text)\n",
    "    \n",
    "    # Package it up as JSON\n",
    "    entities_json = json.dumps(entities)\n",
    "    return entities_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Test cases \n",
    "\n",
    "def test_bbc_scrape():\n",
    "    # Test the BBC scraper with our article\n",
    "    scraper_result = bbc_scraper('https://www.bbc.co.uk/news/uk-52255054')\n",
    "    result_dict = json.loads(scraper_result)\n",
    "    \n",
    "    # Show what we got back\n",
    "    print(\"\\n=== BBC Scraper Test Result ===\")\n",
    "    print(json.dumps(result_dict, indent=2))\n",
    "    \n",
    "    # Check we've got all the bits we need\n",
    "    assert 'URL' in result_dict\n",
    "    assert 'Title' in result_dict\n",
    "    assert 'Date_published' in result_dict\n",
    "    assert 'Content' in result_dict\n",
    "    \n",
    "    # Make sure content is properly formatted without breaks\n",
    "    assert '\\n' not in result_dict['Content']\n",
    "    assert '\\r' not in result_dict['Content']\n",
    "    \n",
    "    # Check the URL matches what we put in\n",
    "    assert result_dict['URL'] == 'https://www.bbc.co.uk/news/uk-52255054'\n",
    "\n",
    "def test_extract_entities_amazon_org():\n",
    "    # Test finding an organisation\n",
    "    input_string = \"I work for Amazon.\"\n",
    "    results_dict = {'people':[],\n",
    "                    'places':[],\n",
    "                    'organisations': ['Amazon']\n",
    "                    }\n",
    "    extracted_entities_results = extract_entities(input_string)\n",
    "    print(\"\\n=== Entity Extraction Test 1 ===\")\n",
    "    print(f\"Input: {input_string}\")\n",
    "    print(\"Result:\", json.dumps(json.loads(extracted_entities_results), indent=2))\n",
    "    assert json.loads(extracted_entities_results) == results_dict\n",
    "\n",
    "def test_extract_entities_name():\n",
    "    # Test finding a person's name\n",
    "    input_string = \"My name is Bob\"\n",
    "    results_dict = {'people':['Bob'],\n",
    "                    'places':[],\n",
    "                    'organisations': []\n",
    "                    }\n",
    "    extracted_entities_results = extract_entities(input_string)\n",
    "    print(\"\\n=== Entity Extraction Test 2 ===\")\n",
    "    print(f\"Input: {input_string}\")\n",
    "    print(\"Result:\", json.dumps(json.loads(extracted_entities_results), indent=2))\n",
    "    assert json.loads(extracted_entities_results) == results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "test_bbc_scrape()\n",
    "test_extract_entities_amazon_org()\n",
    "test_extract_entities_name()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
